"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[7986],{28453:(e,t,a)=>{a.d(t,{R:()=>l,x:()=>s});var n=a(96540);const o={},i=n.createContext(o);function l(e){const t=n.useContext(i);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function s(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:l(e.components),n.createElement(i.Provider,{value:t},e.children)}},45290:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>r,contentTitle:()=>s,default:()=>c,frontMatter:()=>l,metadata:()=>n,toc:()=>u});var n=a(64846),o=a(74848),i=a(28453);const l={title:"How to evaluate a LLM agent?",authors:["liqli","xu"],date:new Date("2024-05-07T00:00:00.000Z")},s=void 0,r={authorsImageUrls:[void 0,void 0]},u=[{value:"The challenges",id:"the-challenges",level:2}];function h(e){const t={h2:"h2",p:"p",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.h2,{id:"the-challenges",children:"The challenges"}),"\n",(0,o.jsx)(t.p,{children:"It is nontrivial to evaluate the performance of a LLM agent.\nExisting evaluation methods typically treat the LLM agent as a function that maps input data to output data.\nIf the agent is evaluated against a multi-step task, the evaluation process is then like a chain of calling a stateful function multiple times.\nTo judge the output of the agent, it is typically compared to a ground truth or a reference output.\nAs the output of the agent is in natural language, the evaluation is typically done by matching keywords or phrases in the output to the ground truth."}),"\n",(0,o.jsx)(t.p,{children:"This evaluation method has its limitations due to its rigid nature.\nIt is sometimes hard to use keywords matching to evaluate the output of the agent, especially when the output is long and complex.\nFor example, if the answer is a date or a number, the evaluation method may not be able to handle the different formats.\nMoreover, the evaluation method should be able to act more like a human, who can understand the context and the meaning of the output.\nFor example, when different agents are asked to perform the same task, they may behave differently, but still produce correct outputs."})]})}function c(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},64846:e=>{e.exports=JSON.parse('{"permalink":"/TaskWeaver/blog/evaluation","editUrl":"https://github.com/microsoft/TaskWeaver/tree/main/website/blog/evaluation.md","source":"@site/blog/evaluation.md","title":"How to evaluate a LLM agent?","description":"The challenges","date":"2024-05-07T00:00:00.000Z","tags":[],"readingTime":6.33,"hasTruncateMarker":true,"authors":[{"name":"Liqun Li","url":"https://liqul.github.io","title":"Principal Researcher","imageURL":"https://liqul.github.io/assets/logo_small_bw.png","key":"liqli","page":null},{"name":"Xu Zhang","url":"https://scholar.google.com/citations?user=bqXdMMMAAAAJ&hl=zh-CN","title":"Senior Researcher","imageURL":"https://scholar.googleusercontent.com/citations?view_op=view_photo&user=bqXdMMMAAAAJ&citpid=3","key":"xu","page":null}],"frontMatter":{"title":"How to evaluate a LLM agent?","authors":["liqli","xu"],"date":"2024-05-07T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Plugins In-Depth","permalink":"/TaskWeaver/blog/plugin"},"nextItem":{"title":"Roles in TaskWeaver","permalink":"/TaskWeaver/blog/role"}}')}}]);