"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[5359],{28453:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>l});var i=r(96540);const o={},s=i.createContext(o);function t(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:t(e.components),i.createElement(s.Provider,{value:n},e.children)}},65938:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"configurations/configurations_in_detail","title":"More about Configurations","description":"More details about important configurations.","source":"@site/docs/configurations/configurations_in_detail.md","sourceDirName":"configurations","slug":"/configurations/configurations_in_detail","permalink":"/TaskWeaver/docs/configurations/configurations_in_detail","draft":false,"unlisted":false,"editUrl":"https://github.com/microsoft/TaskWeaver/tree/main/website/docs/configurations/configurations_in_detail.md","tags":[],"version":"current","frontMatter":{},"sidebar":"documentSidebar","previous":{"title":"Configuration File","permalink":"/TaskWeaver/docs/configurations/overview"},"next":{"title":"Customization","permalink":"/TaskWeaver/docs/customization"}}');var o=r(74848),s=r(28453);const t={},l="More about Configurations",a={},d=[{value:"Planner Configuration",id:"planner-configuration",level:2},{value:"Session Configuration",id:"session-configuration",level:2},{value:"Embedding Configuration",id:"embedding-configuration",level:2},{value:"OpenAI Configuration",id:"openai-configuration",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"more-about-configurations",children:"More about Configurations"})}),"\n",(0,o.jsx)(n.p,{children:"More details about important configurations."}),"\n",(0,o.jsx)(n.h2,{id:"planner-configuration",children:"Planner Configuration"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"planner.example_base_path"}),":\tThe folder to store planner examples. The default value is ",(0,o.jsx)(n.code,{children:"${AppBaseDir}/planner_examples"}),".\nIf you want to create your own planner examples, you can add them to this folder. More details about ",(0,o.jsx)(n.code,{children:"example"})," can referred to ",(0,o.jsx)(n.a,{href:"/TaskWeaver/docs/customization/example/",children:"example"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"planner.prompt_compression"}),": At times, lengthy conversations with the Planner may exceed the input limitations of the LLM model.\nTo address this issue, we can compress the chat history and send it to the LLM model. The default value for this setting is ",(0,o.jsx)(n.code,{children:"false"}),".\nMore details about ",(0,o.jsx)(n.code,{children:"prompt_compression"})," can be referred to ",(0,o.jsx)(n.a,{href:"../advanced/compression",children:"prompt_compression"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"planner.use_experience"}),": Whether to use experience summarized from the previous chat history in planner. The default value is ",(0,o.jsx)(n.code,{children:"false"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"planner.llm_alias"}),": The alias of the LLM used by the Planner. If you do not specify the LLM for the Planner, the primary LLM will be used by default."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"session-configuration",children:"Session Configuration"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"session.max_internal_chat_round_num"}),": the maximum number of internal chat rounds between Planner and Code Interpreter.\nIf the number of internal chat rounds exceeds this number, the session will be terminated.\nThe default value is ",(0,o.jsx)(n.code,{children:"10"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"session.roles"}),": the roles included for the conversation. The default value is ",(0,o.jsx)(n.code,{children:'["planner", "code_interpreter"]'}),".","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"TaskWeaver has 3 code interpreters:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"code_interpreter"}),": it will generate Python code to fulfill the user's request. This is the default code interpreter."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"code_interpreter_plugin_only"}),": please refer to ",(0,o.jsx)(n.a,{href:"/TaskWeaver/docs/plugin/plugin_only",children:"plugin_only_mode"})," for more details."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"code_interpreter_cli_only"}),": allow users to directly communicate with the Command Line Interface (CLI) in natural language.\nPlease refer to ",(0,o.jsx)(n.a,{href:"/TaskWeaver/docs/advanced/cli_only",children:"cli_only_mode"})," for more details."]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["If you do not specify ",(0,o.jsx)(n.code,{children:"planner"}),' in the roles, you will enter the "no-planner" mode.\nIt allows users to directly communicate with the worker role, such as ',(0,o.jsx)(n.code,{children:"code_interpreter"}),".\nIn this mode, users can only send messages to the ",(0,o.jsx)(n.code,{children:"CodeInterpreter"})," and receive messages from the ",(0,o.jsx)(n.code,{children:"CodeInterpreter"}),'.\nNote that only single worker role is allowed in the "no-planner" mode because all user requests will be sent to the worker role directly.\nHere is an example:']}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:" =========================================================\n _____         _     _       __\n|_   _|_ _ ___| | _ | |     / /__  ____ __   _____  _____\n  | |/ _` / __| |/ /| | /| / / _ \\/ __ `/ | / / _ \\/ ___/\n  | | (_| \\__ \\   < | |/ |/ /  __/ /_/ /| |/ /  __/ /\n  |_|\\__,_|___/_|\\_\\|__/|__/\\___/\\__,_/ |___/\\___/_/\n=========================================================\nTaskWeaver: I am TaskWeaver, an AI assistant. To get started, could you please enter your request?\nHuman: generate 10 random numbers\n>>> [PYTHON]Starting... \nimport numpy as np\nrandom_numbers = np.random.rand(10)\nrandom_numbers\n>>> [VERIFICATION]\nNONE\n>>> [STATUS]Starting...         \nSUCCESS\n>>> [RESULT]\nThe execution of the generated python code above has succeeded\n\nThe result of above Python code after execution is:\narray([0.09918602, 0.68732778, 0.44413814, 0.4756623 , 0.48302334,\n       0.8286594 , 0.80994359, 0.35677263, 0.45719317, 0.68240194])\n>>> [CODEINTERPRETER->PLANNER]\nThe following python code has been executed:\n```python\nimport numpy as np\nrandom_numbers = np.random.rand(10)\nrandom_numbers\n```\n\nThe execution of the generated python code above has succeeded\n\nThe result of above Python code after execution is:\narray([0.09918602, 0.68732778, 0.44413814, 0.4756623 , 0.48302334,\n       0.8286594 , 0.80994359, 0.35677263, 0.45719317, 0.68240194])\nTaskWeaver: The following python code has been executed:\n```python\nimport numpy as np\nrandom_numbers = np.random.rand(10)\nrandom_numbers\n```\n\nThe execution of the generated python code above has succeeded\n\nThe result of above Python code after execution is:\narray([0.09918602, 0.68732778, 0.44413814, 0.4756623 , 0.48302334,\n       0.8286594 , 0.80994359, 0.35677263, 0.45719317, 0.68240194])\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"embedding-configuration",children:"Embedding Configuration"}),"\n",(0,o.jsx)(n.p,{children:"In TaskWeaver, we support various embedding models to generate embeddings for auto plugin selection."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"llm.embedding_api_type"}),": The type of the embedding API. We support the following types:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"openai"}),"\n",(0,o.jsx)(n.li,{children:"qwen"}),"\n",(0,o.jsx)(n.li,{children:"ollama"}),"\n",(0,o.jsx)(n.li,{children:"sentence_transformers"}),"\n",(0,o.jsx)(n.li,{children:"glm"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"llm.embedding_model"}),": The embedding model name. The model name should be aligned with ",(0,o.jsx)(n.code,{children:"llm.embedding_api_type"}),".\nWe only list some embedding models we have tested below:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["openai","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"text-embedding-ada-002"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["qwen","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"text-embedding-v1"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["ollama","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"llama2"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["sentence_transformers","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"all-mpnet-base-v2"}),"\n",(0,o.jsx)(n.li,{children:"multi-qa-mpnet-base-dot-v1"}),"\n",(0,o.jsx)(n.li,{children:"all-distilroberta-v1"}),"\n",(0,o.jsx)(n.li,{children:"all-MiniLM-L12-v2"}),"\n",(0,o.jsx)(n.li,{children:"multi-qa-MiniLM-L6-cos-v1"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["zhipuai","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"embedding-2\nYou also can use other embedding models supported by the above embedding APIs."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"openai-configuration",children:"OpenAI Configuration"}),"\n",(0,o.jsx)(n.p,{children:"Today, more and more inference frameworks support OpenAI compatible APIs. However, different models\nmay have different configurations. Here are some supported configurations for other models adapted\nfor OpenAI API."}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"llm.openai.support_system_role"}),": Whether to support system role in the conversation. The default value is ",(0,o.jsx)(n.code,{children:"True"}),". For\nthe models that do not support system role, you can set this value to ",(0,o.jsx)(n.code,{children:"False"}),", and the system role will be replaced by the user role."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"llm.openai.require_alternative_roles"}),": Whether to require alternative roles in the conversation. The default value is ",(0,o.jsx)(n.code,{children:"False"}),".\nWe notice that some models may require exactly alternative roles in the conversation. If you set this value to ",(0,o.jsx)(n.code,{children:"True"}),", the system will\ncheck consecutive ",(0,o.jsx)(n.code,{children:"user"})," messages in the conversation history. If there is, the system will add a dummy ",(0,o.jsx)(n.code,{children:"assistant"})," message in between."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"llm.openai.support_constrained_generation"}),": Whether to support constrained generation in the conversation. The default value is ",(0,o.jsx)(n.code,{children:"False"}),".\nSome inferencing frameworks like ",(0,o.jsx)(n.a,{href:"https://docs.vllm.ai/en/stable/serving/openai_compatible_server.html",children:"vllm"})," and ",(0,o.jsx)(n.a,{href:"https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#constrained-output-with-grammars",children:"llama.cpp"}),"\nsupport constrained generation. Currently, we only support vllm. If you want to use this feature, you can set this value to ",(0,o.jsx)(n.code,{children:"True"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"llm.openai.json_schema_enforcer"}),": This is configured together with ",(0,o.jsx)(n.code,{children:"llm.openai.support_constrained_generation"}),". If you want to use\nconstrained generation. There are two valid options: ",(0,o.jsx)(n.code,{children:"lm-format-enforcer"})," and ",(0,o.jsx)(n.code,{children:"outlines"}),"."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}}}]);