"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1592],{28453:(e,n,l)=>{l.d(n,{R:()=>o,x:()=>r});var a=l(96540);const s={},t=a.createContext(s);function o(e){const n=a.useContext(t);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),a.createElement(t.Provider,{value:n},e.children)}},75152:(e,n,l)=>{l.r(n),l.d(n,{assets:()=>i,contentTitle:()=>r,default:()=>m,frontMatter:()=>o,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"llms/ollama","title":"Ollama","description":"1. Go to Ollama and follow the instructions to serve a LLM model on your local environment.","source":"@site/docs/llms/ollama.md","sourceDirName":"llms","slug":"/llms/ollama","permalink":"/TaskWeaver/docs/llms/ollama","draft":false,"unlisted":false,"editUrl":"https://github.com/microsoft/TaskWeaver/tree/main/website/docs/llms/ollama.md","tags":[],"version":"current","frontMatter":{},"sidebar":"documentSidebar","previous":{"title":"LiteLLM","permalink":"/TaskWeaver/docs/llms/liteLLM"},"next":{"title":"Gemini","permalink":"/TaskWeaver/docs/llms/gemini"}}');var s=l(74848),t=l(28453);const o={},r="Ollama",i={},c=[];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"ollama",children:"Ollama"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Go to ",(0,s.jsx)(n.a,{href:"https://github.com/jmorganca/ollama",children:"Ollama"})," and follow the instructions to serve a LLM model on your local environment.\nWe provide a short example to show how to configure the ollama in the following, which might change if ollama makes updates."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",metastring:'title="install ollama and serve LLMs in local" showLineNumbers',children:"## Install ollama on Linux & WSL2\ncurl https://ollama.ai/install.sh | sh\n## Run the serving\nollama serve\n"})}),"\n",(0,s.jsx)(n.p,{children:"Open another terminal and run:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ollama run llama2:13b\n"})}),"\n",(0,s.jsx)(n.admonition,{type:"tip",children:(0,s.jsx)(n.p,{children:"We recommend deploying the LLM with a parameter scale exceeding 13B for enhanced performance (such as Llama 2 13B)."})}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsxs)(n.p,{children:["When serving LLMs via Ollama, it will by default start a server at ",(0,s.jsx)(n.code,{children:"http://localhost:11434"}),", which will later be used as the API base in ",(0,s.jsx)(n.code,{children:"taskweaver_config.json"}),"."]})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsxs)(n.li,{children:["Add following configuration to ",(0,s.jsx)(n.code,{children:"taskweaver_config.json"}),":"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",metastring:"showLineNumbers",children:'{\n    "llm.api_base": "http://localhost:11434",\n    "llm.api_key": "ARBITRARY_STRING",\n    "llm.api_type": "ollama",\n    "llm.model": "llama2:13b"\n}\n'})}),"\n",(0,s.jsxs)(n.p,{children:["NOTE: ",(0,s.jsx)(n.code,{children:"llm.api_base"})," is the URL started in the Ollama LLM server and ",(0,s.jsx)(n.code,{children:"llm.model"})," is the model name of Ollama LLM, it should be same as the one you served before."]}),"\n",(0,s.jsxs)(n.ol,{start:"3",children:["\n",(0,s.jsxs)(n.li,{children:["Start TaskWeaver and chat with TaskWeaver.\nYou can refer to the ",(0,s.jsx)(n.a,{href:"/TaskWeaver/docs/quickstart",children:"Quick Start"})," for more details."]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);